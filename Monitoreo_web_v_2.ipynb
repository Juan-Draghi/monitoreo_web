{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtXNntEn+gIxD4sHdLLqNt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juan-Draghi/monitoreo_web/blob/main/Monitoreo_web_v_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Script para monitorear cambios en sitios web (versión 2)**\n",
        "**¿Qué hace esta versión?**\n",
        "\n",
        "1. Nueva carpeta para hashes: Guarda los hashes en una carpeta diferente en Google Drive (web_monitoring_hashes_text) para distinguirlos de los hashes del HTML completo.\n",
        "2. Función extract_visible_text: Utiliza BeautifulSoup para obtener solo el texto visible de la página, eliminando todas las etiquetas HTML.\n",
        "3. Comparación basada en el texto: El hash ahora se calcula sobre este texto visible, y la comparación se realiza entre los hashes del texto."
      ],
      "metadata": {
        "id": "f3tJ3Eyoc_bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import hashlib\n",
        "import os\n",
        "from google.colab import drive\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- Configuración ---\n",
        "urls_a_monitorear = [\n",
        "    \"https://buenosaires.gob.ar/jefaturadegabinete/desarrollo-urbano/normativa/codigo-urbanistico-y-de-edificacion\",\n",
        "    \"https://buenosaires.gob.ar/jefaturadegabinete/desarrollo-urbano/novedades-de-la-subsecretaria-de-gestion-urbana\",\n",
        "    \"https://buenosaires.gob.ar/agc/noticias\",\n",
        "    \"https://www.cpau.org/noticias\",\n",
        "    \"https://buenosaires.gob.ar/noticias/desarrollo-urbano\",\n",
        "    \"https://adrianmercadorealestate.com/blog/informes\",\n",
        "    \"https://www.estadisticaciudad.gob.ar/eyc/?page_id=1479\",\n",
        "    \"https://www.ceso.com.ar/secciones/provincias-y-regiones\",\n",
        "    \"https://colegioinmobiliario.org.ar/institucional/observatorioEstadistico\",\n",
        "    \"https://colegioinmobiliario.org.ar/novedades\",\n",
        "    \"https://www.colliers.com/es-ar\",\n",
        "    \"https://www.afcp.org.ar/despacho-mensual\",\n",
        "    \"https://www.indec.gob.ar/indec/web/Nivel3-Tema-3-3\",\n",
        "    \"https://www.remax.com.ar/indice-remax\",\n",
        "    \"https://www.ieric.org.ar/estadistica/informes-de-coyuntura/?2025\",\n",
        "    \"https://www.uade.edu.ar/sites/investigacion/instituto-de-economia-ineco/informes-y-novedades/mei-informe-del-mercado-inmobiliario-e-indice-del-salario-real/\",\n",
        "    \"https://www.nmrk.com.ar/informe/11023/reportes-de-mercado-2025\",\n",
        "    \"https://www.fabianachaval.com/blog\",\n",
        "    \"https://sites.google.com/view/red-operaciones-inmobiliarias\",\n",
        "    \"https://www.ljramos.com.ar/informes-del-mercado-inmobiliario\",\n",
        "    \"https://www.zonaprop.com.ar/noticias/zpindex/\",\n",
        "    \"https://www.zonaprop.com.ar/noticias/zpindex/gba-venta/\",\n",
        "    \"https://www.zonaprop.com.ar/noticias/zpindex/gba-oeste-sur-venta/\",\n",
        "    \"https://colegioinmobiliario.org.ar/novedades\",\n",
        "    \"https://www.cpau.org/noticias\",\n",
        "    \"https://noticias.argenprop.com/\"\n",
        "]\n",
        "drive_base_path = '/content/drive/MyDrive/web_monitoring_hashes_text' # Nueva carpeta para hashes de texto\n",
        "\n",
        "# --- Funciones ---\n",
        "\n",
        "def get_page_content(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.9',\n",
        "        'Referer': url\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error al obtener {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_visible_text(html_content):\n",
        "    if not html_content:\n",
        "        return \"\"\n",
        "    try:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        # Obtener todo el texto visible, separando con espacios y eliminando espacios extra\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error al extraer texto: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def calculate_hash(text):\n",
        "    if not text:\n",
        "        return \"d41d8cd98f00b204e9800998ecf8427e\"\n",
        "    return hashlib.sha256(text.encode('utf-8', errors='ignore')).hexdigest()\n",
        "\n",
        "def get_filename_from_url(url):\n",
        "    url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()\n",
        "    return f\"{url_hash}_text.txt\" # Sufijo para distinguir de hashes de HTML\n",
        "\n",
        "def save_last_hash(file_path, current_hash):\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "        with open(file_path, 'w') as f:\n",
        "            f.write(current_hash)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar hash en {file_path}: {e}\")\n",
        "\n",
        "def load_last_hash(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                return f.read().strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Error al cargar hash desde {file_path}: {e}\")\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "# --- Ejecución principal ---\n",
        "\n",
        "print(\"Montando Google Drive...\")\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive montado.\")\n",
        "    drive_mounted = True\n",
        "except Exception as e:\n",
        "    print(f\"Error al montar Google Drive: {e}\")\n",
        "    print(\"No se podrá guardar/cargar el estado anterior.\")\n",
        "    drive_mounted = False\n",
        "\n",
        "print(f\"\\nIniciando monitoreo de {len(urls_a_monitorear)} URL(s) (comparando texto visible)...\")\n",
        "cambios_detectados = []\n",
        "urls_con_error = []\n",
        "\n",
        "for url in urls_a_monitorear:\n",
        "    print(f\"\\n--- Procesando: {url} ---\")\n",
        "    html_content = get_page_content(url)\n",
        "\n",
        "    if html_content is None:\n",
        "        print(f\"No se pudo obtener contenido de {url}. Saltando.\")\n",
        "        urls_con_error.append(url)\n",
        "        continue\n",
        "\n",
        "    visible_text = extract_visible_text(html_content)\n",
        "    current_hash = calculate_hash(visible_text)\n",
        "\n",
        "    if not current_hash:\n",
        "        print(f\"No se pudo calcular el hash del texto de {url}. Saltando.\")\n",
        "        urls_con_error.append(url)\n",
        "        continue\n",
        "\n",
        "    if drive_mounted:\n",
        "        url_state_filename = get_filename_from_url(url)\n",
        "        url_state_path = os.path.join(drive_base_path, url_state_filename)\n",
        "        last_hash = load_last_hash(url_state_path)\n",
        "\n",
        "        if last_hash is None:\n",
        "            print(\"  Estado anterior no encontrado. Guardando hash del texto actual.\")\n",
        "            save_last_hash(url_state_path, current_hash)\n",
        "        elif current_hash != last_hash:\n",
        "            print(\"  ¡CAMBIO DETECTADO en el texto visible!\")\n",
        "            cambios_detectados.append(url)\n",
        "            save_last_hash(url_state_path, current_hash)\n",
        "        else:\n",
        "            print(\"  No se detectaron cambios en el texto visible.\")\n",
        "    else:\n",
        "        print(\"  Google Drive no está montado. No se puede comparar el estado anterior.\")\n",
        "\n",
        "print(\"\\n--- Resumen del Monitoreo ---\")\n",
        "if cambios_detectados:\n",
        "    print(f\"Se detectaron cambios en el texto visible de las siguientes {len(cambios_detectados)} URL(s):\")\n",
        "    for url_cambiada in cambios_detectados:\n",
        "        print(f\"- {url_cambiada}\")\n",
        "else:\n",
        "    print(\"No se detectaron cambios en el texto visible de las URLs procesadas.\")\n",
        "\n",
        "if urls_con_error:\n",
        "    print(f\"\\nOcurrieron errores al procesar las siguientes {len(urls_con_error)} URL(s):\")\n",
        "    for url_error in urls_con_error:\n",
        "        print(f\"- {url_error}\")\n",
        "\n",
        "# Desmontar Drive al finalizar (opcional)\n",
        "# drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "DDkalKaqUHRG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}