{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Script para monitorear cambios en sitios web**"
      ],
      "metadata": {
        "id": "f3tJ3Eyoc_bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import hashlib\n",
        "import os\n",
        "from google.colab import drive\n",
        "import re\n",
        "import ssl\n",
        "from bs4 import BeautifulSoup, Comment # Importar BeautifulSoup y Comment\n",
        "\n",
        "# --- Configuración ---\n",
        "# Lista de URLs a monitorear\n",
        "urls_a_monitorear = [\n",
        "    \"https://buenosaires.gob.ar/jefaturadegabinete/desarrollo-urbano/normativa/codigo-urbanistico-y-de-edificacion\",\n",
        "    \"https://buenosaires.gob.ar/jefaturadegabinete/desarrollo-urbano/novedades-de-la-subsecretaria-de-gestion-urbana\",\n",
        "    \"https://buenosaires.gob.ar/agc/noticias\",\n",
        "    \"https://buenosaires.gob.ar/noticias/desarrollo-urbano\",\n",
        "    \"https://adrianmercadorealestate.com/blog/informes\",\n",
        "    \"https://www.estadisticaciudad.gob.ar/eyc/?page_id=1479\",\n",
        "    \"https://www.ceso.com.ar/secciones/provincias-y-regiones\",\n",
        "    \"https://colegioinmobiliario.org.ar/institucional/observatorioEstadistico\",\n",
        "    \"https://www.colliers.com/es-ar\",\n",
        "    \"https://www.afcp.org.ar/despacho-mensual\",\n",
        "    \"https://www.indec.gob.ar/indec/web/Nivel3-Tema-3-3\",\n",
        "    \"https://www.remax.com.ar/indice-remax\",\n",
        "    \"https://www.ieric.org.ar/estadistica/informes-de-coyuntura/?2025\",\n",
        "    \"https://www.uade.edu.ar/sites/investigacion/instituto-de-economia-ineco/informes-y-novedades/mei-informe-del-mercado-inmobiliario-e-indice-del-salario-real/\",\n",
        "    \"https://www.nmrk.com.ar/informe/11023/reportes-de-mercado-2025\",\n",
        "    \"https://www.fabianachaval.com/blog\",\n",
        "    \"https://sites.google.com/view/red-operaciones-inmobiliarias\",\n",
        "    \"https://www.ljramos.com.ar/informes-del-mercado-inmobiliario\",\n",
        "    \"https://www.zonaprop.com.ar/noticias/zpindex/\",\n",
        "    \"https://www.zonaprop.com.ar/noticias/zpindex/gba-venta/\",\n",
        "    \"https://www.zonaprop.com.ar/noticias/zpindex/gba-oeste-sur-venta/\",\n",
        "    \"https://colegioinmobiliario.org.ar/novedades\",\n",
        "    \"https://www.cpau.org/noticias\",\n",
        "    \"https://noticias.argenprop.com/\"\n",
        "]\n",
        "\n",
        "# Directorio en Google Drive para guardar los hashes de los estados anteriores\n",
        "# Asegúrate de que la carpeta 'web_monitoring_hashes' exista en la raíz de tu Drive\n",
        "drive_base_path = '/content/drive/MyDrive/web_monitoring_hashes'\n",
        "\n",
        "# --- Funciones ---\n",
        "\n",
        "def get_page_content(url):\n",
        "    \"\"\"Obtiene el contenido HTML de una URL, manejando errores comunes.\"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.9',\n",
        "        'Referer': url\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        print(f\"  Contenido obtenido exitosamente.\")\n",
        "        return response.text\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"  Error HTTP para {url}: {e}\")\n",
        "        if e.response.status_code == 403:\n",
        "            print(\"    Posiblemente bloqueado por medidas anti-bot.\")\n",
        "        return None\n",
        "\n",
        "    except requests.exceptions.SSLError as e:\n",
        "        print(f\"  Error SSL para {url}: {e}\")\n",
        "        print(\"    Puede haber un problema con el certificado del sitio o la verificación.\")\n",
        "        # --- INICIO: Código PELIGROSO (Desactivar verificación SSL) ---\n",
        "        # Si ACEPTAS EL RIESGO para esta URL específica, puedes intentar descomentar:\n",
        "        try:\n",
        "            print(\"    Intentando sin verificar certificado SSL (NO SEGURO!)...\")\n",
        "            response = requests.get(url, headers=headers, timeout=15, verify=False)\n",
        "            response.raise_for_status()\n",
        "            print(f\"    Contenido obtenido (sin verificación SSL).\")\n",
        "            return response.text\n",
        "        except requests.exceptions.RequestException as inner_e:\n",
        "            print(f\"    Falló incluso sin verificación SSL: {inner_e}\")\n",
        "            return None\n",
        "        # --- FIN: Código PELIGROSO ---\n",
        "        return None\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"  Otro error de conexión/solicitud para {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Ocurrió un error inesperado al procesar {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def clean_html_for_hashing(html_content):\n",
        "    \"\"\"\n",
        "    Limpia el contenido HTML para reducir la sensibilidad al ruido.\n",
        "    Elimina scripts, styles, comentarios y algunos metadatos no esenciales.\n",
        "    Normaliza espacios en blanco.\n",
        "    \"\"\"\n",
        "    if not html_content:\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Eliminar elementos de script y style\n",
        "        for script_or_style in soup([\"script\", \"style\"]):\n",
        "            script_or_style.decompose()\n",
        "\n",
        "        # Eliminar comentarios\n",
        "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
        "            comment.extract()\n",
        "\n",
        "        # Opcional: Eliminar ciertos metadatos que cambian a menudo (ej. generador, viewport si no son importantes)\n",
        "        # Cuidado con esto, algunos meta tags son importantes (ej. charset, description)\n",
        "        for meta in soup.find_all('meta'):\n",
        "            if meta.get('name') in ['generator', 'viewport']:\n",
        "                meta.decompose()\n",
        "        #     # Podrías añadir otras condiciones si identificas meta tags ruidosos\n",
        "\n",
        "        # Opcional: Eliminar elementos específicos ruidosos (ej. contadores de visitas discretos)\n",
        "        if soup.find('div', id='visitor-counter'):\n",
        "            soup.find('div', id='visitor-counter').decompose()\n",
        "\n",
        "        # Convertir el objeto BeautifulSoup limpio de vuelta a una cadena.\n",
        "        # Esto preserva la estructura y los atributos de los elementos restantes.\n",
        "        # El uso de formatter=\"html\" y pretty_print=False ayuda a obtener una salida consistente\n",
        "        # aunque puede variar ligeramente entre versiones de bs4 o parsers.\n",
        "        # Usar .encode().decode() y lstrip() ayuda a normalizar la salida de bs4\n",
        "        cleaned_html = str(soup.prettify(formatter=None)).strip()\n",
        "\n",
        "\n",
        "        # Opcional: Si solo te interesa el texto visible (ignorar atributos, estructura)\n",
        "        text_content = soup.get_text(separator=' ', strip=True)\n",
        "        return text_content\n",
        "        # PERO, hashing el HTML limpio es mejor para detectar cambios como alt text, href, etc.\n",
        "\n",
        "        return cleaned_html\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error durante la limpieza del HTML: {e}\")\n",
        "        return \"\" # Retorna vacío si la limpieza falla\n",
        "\n",
        "def calculate_hash(content):\n",
        "    \"\"\"Calcula el hash SHA256 del contenido.\"\"\"\n",
        "    if not content: # Manejar contenido vacío después de la limpieza o por error\n",
        "        return \"d41d8cd98f00b204e9800998ecf8427e\" # Hash de una cadena vacía (MD5), o SHA256 de vacío: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
        "    return hashlib.sha256(content.encode('utf-8', errors='ignore')).hexdigest()\n",
        "\n",
        "def get_filename_from_url(url):\n",
        "    \"\"\"Genera un nombre de archivo seguro basado en el hash de la URL.\"\"\"\n",
        "    url_hash = hashlib.md5(url.encode('utf-8')).hexdigest() # Usar MD5 para el nombre del archivo\n",
        "    return f\"{url_hash}.txt\"\n",
        "\n",
        "def save_last_hash(file_path, current_hash):\n",
        "    \"\"\"Guarda el hash actual en un archivo.\"\"\"\n",
        "    if not current_hash: # No guardar hash si el cálculo falló\n",
        "         return\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "        with open(file_path, 'w') as f:\n",
        "            f.write(current_hash)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar el hash en {file_path}: {e}\")\n",
        "\n",
        "def load_last_hash(file_path):\n",
        "    \"\"\"Carga el último hash guardado desde un archivo.\"\"\"\n",
        "    if os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                return f.read().strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Error al cargar el hash desde {file_path}: {e}\")\n",
        "            return None\n",
        "    return None # No existe el archivo\n",
        "\n",
        "# --- Ejecución principal ---\n",
        "\n",
        "# 1. Montar Google Drive\n",
        "print(\"Montando Google Drive...\")\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive montado.\")\n",
        "    drive_mounted = True\n",
        "except Exception as e:\n",
        "    print(f\"Error al montar Google Drive: {e}\")\n",
        "    print(\"No se podrá guardar/cargar el estado anterior. Los cambios solo se detectarán en esta ejecución.\")\n",
        "    drive_mounted = False\n",
        "\n",
        "\n",
        "# 2. Procesar cada URL\n",
        "print(f\"\\nIniciando monitoreo de {len(urls_a_monitorear)} URL(s)...\")\n",
        "cambios_detectados = []\n",
        "urls_con_error = []\n",
        "\n",
        "for url in urls_a_monitorear:\n",
        "    print(f\"\\n--- Procesando: {url} ---\")\n",
        "    current_content = get_page_content(url)\n",
        "\n",
        "    if current_content is None:\n",
        "        print(f\"No se pudo obtener contenido de {url}. Saltando.\")\n",
        "        urls_con_error.append(url)\n",
        "        continue\n",
        "\n",
        "    # --- Limpiar contenido antes de hashear ---\n",
        "    cleaned_content = clean_html_for_hashing(current_content)\n",
        "\n",
        "    if not cleaned_content: # Si la limpieza resultó en contenido vacío (podría pasar si la página estaba vacía o la limpieza falló)\n",
        "         print(f\"Contenido limpio de {url} está vacío. No se puede monitorear.\")\n",
        "         urls_con_error.append(url) # Considerar como error si no hay contenido limpio\n",
        "         continue\n",
        "\n",
        "    current_hash = calculate_hash(cleaned_content)\n",
        "    if current_hash is None: # Aunque calculate_hash maneja vacío, mantener por si acaso\n",
        "        print(f\"No se pudo calcular el hash de {url}. Saltando.\")\n",
        "        urls_con_error.append(url)\n",
        "        continue\n",
        "\n",
        "    if drive_mounted:\n",
        "        url_state_filename = get_filename_from_url(url)\n",
        "        url_state_path = os.path.join(drive_base_path, url_state_filename)\n",
        "\n",
        "        last_hash = load_last_hash(url_state_path)\n",
        "\n",
        "        # 3. Comparar y reportar\n",
        "        if last_hash is None:\n",
        "            print(\"  Estado anterior no encontrado (primera vez monitoreando o archivo eliminado). Guardando estado actual limpio.\")\n",
        "            save_last_hash(url_state_path, current_hash)\n",
        "        elif current_hash != last_hash:\n",
        "            print(\"  ¡CAMBIO DETECTADO!\")\n",
        "            cambios_detectados.append(url)\n",
        "            save_last_hash(url_state_path, current_hash) # Actualizar el estado guardado\n",
        "        else:\n",
        "            print(\"  No se detectaron cambios.\")\n",
        "    else:\n",
        "         print(\"  Google Drive no está montado. No se puede comparar con el estado anterior ni guardar el actual.\")\n",
        "\n",
        "\n",
        "# 4. Resumen de resultados\n",
        "print(\"\\n--- Resumen del Monitoreo ---\")\n",
        "if cambios_detectados:\n",
        "    print(f\"Se detectaron cambios en las siguientes {len(cambios_detectados)} URL(s):\")\n",
        "    for url_cambiada in cambios_detectados:\n",
        "        print(f\"- {url_cambiada}\")\n",
        "else:\n",
        "    print(\"No se detectaron cambios en las URLs procesadas (o era la primera ejecución para algunas, o hubo errores).\")\n",
        "\n",
        "if urls_con_error:\n",
        "    print(f\"\\nOcurrieron errores o problemas de contenido al procesar las siguientes {len(urls_con_error)} URL(s):\")\n",
        "    for url_error in urls_con_error:\n",
        "        print(f\"- {url_error}\")\n",
        "    print(\"\\nRevisa los mensajes de error detallados arriba para cada una de estas URLs.\")\n",
        "\n",
        "# Desmontar Drive al finalizar (opcional)\n",
        "# print(\"\\nDesmontando Google Drive...\")\n",
        "# drive.flush_and_unmount()\n",
        "# print(\"Google Drive desmontado.\")"
      ],
      "metadata": {
        "id": "vhvJyTQBir29"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}